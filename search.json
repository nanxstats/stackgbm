[{"path":[]},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"our-pledge","dir":"","previous_headings":"","what":"Our Pledge","title":"Contributor Covenant Code of Conduct","text":"members, contributors, leaders pledge make participation community harassment-free experience everyone, regardless age, body size, visible invisible disability, ethnicity, sex characteristics, gender identity expression, level experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, sexual identity orientation. pledge act interact ways contribute open, welcoming, diverse, inclusive, healthy community.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"our-standards","dir":"","previous_headings":"","what":"Our Standards","title":"Contributor Covenant Code of Conduct","text":"Examples behavior contributes positive environment community include: Demonstrating empathy kindness toward people respectful differing opinions, viewpoints, experiences Giving gracefully accepting constructive feedback Accepting responsibility apologizing affected mistakes, learning experience Focusing best just us individuals, overall community Examples unacceptable behavior include: use sexualized language imagery, sexual attention advances kind Trolling, insulting derogatory comments, personal political attacks Public private harassment Publishing others‚Äô private information, physical email address, without explicit permission conduct reasonably considered inappropriate professional setting","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"enforcement-responsibilities","dir":"","previous_headings":"","what":"Enforcement Responsibilities","title":"Contributor Covenant Code of Conduct","text":"Community leaders responsible clarifying enforcing standards acceptable behavior take appropriate fair corrective action response behavior deem inappropriate, threatening, offensive, harmful. Community leaders right responsibility remove, edit, reject comments, commits, code, wiki edits, issues, contributions aligned Code Conduct, communicate reasons moderation decisions appropriate.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"scope","dir":"","previous_headings":"","what":"Scope","title":"Contributor Covenant Code of Conduct","text":"Code Conduct applies within community spaces, also applies individual officially representing community public spaces. Examples representing community include using official e-mail address, posting via official social media account, acting appointed representative online offline event.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"enforcement","dir":"","previous_headings":"","what":"Enforcement","title":"Contributor Covenant Code of Conduct","text":"Instances abusive, harassing, otherwise unacceptable behavior may reported community leaders responsible enforcement @nanx.. complaints reviewed investigated promptly fairly. community leaders obligated respect privacy security reporter incident.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"enforcement-guidelines","dir":"","previous_headings":"","what":"Enforcement Guidelines","title":"Contributor Covenant Code of Conduct","text":"Community leaders follow Community Impact Guidelines determining consequences action deem violation Code Conduct:","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"id_1-correction","dir":"","previous_headings":"Enforcement Guidelines","what":"1. Correction","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Use inappropriate language behavior deemed unprofessional unwelcome community. Consequence: private, written warning community leaders, providing clarity around nature violation explanation behavior inappropriate. public apology may requested.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"id_2-warning","dir":"","previous_headings":"Enforcement Guidelines","what":"2. Warning","title":"Contributor Covenant Code of Conduct","text":"Community Impact: violation single incident series actions. Consequence: warning consequences continued behavior. interaction people involved, including unsolicited interaction enforcing Code Conduct, specified period time. includes avoiding interactions community spaces well external channels like social media. Violating terms may lead temporary permanent ban.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"id_3-temporary-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"3. Temporary Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: serious violation community standards, including sustained inappropriate behavior. Consequence: temporary ban sort interaction public communication community specified period time. public private interaction people involved, including unsolicited interaction enforcing Code Conduct, allowed period. Violating terms may lead permanent ban.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"id_4-permanent-ban","dir":"","previous_headings":"Enforcement Guidelines","what":"4. Permanent Ban","title":"Contributor Covenant Code of Conduct","text":"Community Impact: Demonstrating pattern violation community standards, including sustained inappropriate behavior, harassment individual, aggression toward disparagement classes individuals. Consequence: permanent ban sort public interaction within community.","code":""},{"path":"https://nanx.me/stackgbm/CODE_OF_CONDUCT.html","id":"attribution","dir":"","previous_headings":"","what":"Attribution","title":"Contributor Covenant Code of Conduct","text":"Code Conduct adapted Contributor Covenant, version 2.1, available https://www.contributor-covenant.org/version/2/1/code_of_conduct.html. Community Impact Guidelines inspired [Mozilla‚Äôs code conduct enforcement ladder][https://github.com/mozilla/inclusion]. answers common questions code conduct, see FAQ https://www.contributor-covenant.org/faq. Translations available https://www.contributor-covenant.org/translations.","code":""},{"path":"https://nanx.me/stackgbm/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to stackgbm","title":"Contributing to stackgbm","text":"üëçüéâ First , thanks taking time contribute! üéâüëç contribute project : Filing bug report feature request issue. Suggesting change via pull request.","code":""},{"path":"https://nanx.me/stackgbm/CONTRIBUTING.html","id":"issues","dir":"","previous_headings":"","what":"Issues","title":"Contributing to stackgbm","text":"file issue possible bug, please try include: Relevant package versions Necessary code data reproduce issue","code":""},{"path":"https://nanx.me/stackgbm/CONTRIBUTING.html","id":"pull-requests","dir":"","previous_headings":"","what":"Pull requests","title":"Contributing to stackgbm","text":"suggest change via pull requests, please: Fork repository GitHub account. Clone forked repository local machine, make changes. Commit push changes GitHub. Create pull request.","code":""},{"path":"https://nanx.me/stackgbm/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 stackgbm authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (‚ÄúSoftware‚Äù), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED ‚Äú‚Äù, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model stacking for boosted trees","text":"Model stacking (Wolpert 1992) method ensemble learning combines strength multiple base learners drive predictive performance. particularly popular effective strategy used machine learning competitions. stackgbm implements two-layer stacking model: first layer generates ‚Äúfeatures‚Äù produced gradient boosting trees. boosted tree models built xgboost (Chen Guestrin 2016), lightgbm (Ke et al. 2017), catboost (Prokhorenkova et al. 2018). second layer logistic regression uses features inputs.","code":"library(\"stackgbm\")"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"generate-data","dir":"Articles","previous_headings":"","what":"Generate data","title":"Model stacking for boosted trees","text":"Let‚Äôs generate data demonstrate purposes. simulated data \\(1000 \\times 50\\) predictor matrix binary outcome vector. 800 samples training set rest 200 (independent) test set. 25 50 features informative follows \\(N(0, 10)\\).","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 1000,   p = 50,   rho = 0.6,   coef = rnorm(25, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  x_train <- sim_data$x.tr x_test <- sim_data$x.te y_train <- as.vector(sim_data$y.tr) y_test <- as.vector(sim_data$y.te)"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"parameter-tuning","dir":"Articles","previous_headings":"","what":"Parameter tuning","title":"Model stacking for boosted trees","text":"cv_xgboost(), cv_lightgbm() cv_catboost() provide wrappers tuning essential hyperparameters type boosted tree models k-fold cross-validation. ‚Äúoptimal‚Äù parameters used fit stacking model later.","code":"params_xgboost <- cv_xgboost(x_train, y_train) params_lightgbm <- cv_lightgbm(x_train, y_train) params_catboost <- cv_catboost(x_train, y_train)"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"train-the-stackgbm-model","dir":"Articles","previous_headings":"","what":"Train the stackgbm model","title":"Model stacking for boosted trees","text":"","code":"model_stackgbm <- stackgbm(   sim_data$x.tr,   sim_data$y.tr,   params = list(     params_xgboost,     params_lightgbm,     params_catboost   ) )"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"inference","dir":"Articles","previous_headings":"","what":"Inference","title":"Model stacking for boosted trees","text":"","code":"roc_stackgbm_train <- pROC::roc(   y_train,   predict(model_stackgbm, x_train)$prob,   quiet = TRUE ) roc_stackgbm_test <- pROC::roc(   y_test,   predict(model_stackgbm, x_test)$prob,   quiet = TRUE ) roc_stackgbm_train$auc #> Area under the curve: 0.9663 roc_stackgbm_test$auc #> Area under the curve: 0.7835"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"performance-evaluation","dir":"Articles","previous_headings":"","what":"Performance evaluation","title":"Model stacking for boosted trees","text":"Let‚Äôs compare predictive performance stacking model three types tree boosting models (base learners) fitted individually.","code":"model_xgboost <- xgboost_train(   params = list(     objective = \"binary:logistic\",     eval_metric = \"auc\",     max_depth = params_xgboost$max_depth,     eta = params_xgboost$eta   ),   data = xgboost_dmatrix(x_train, label = y_train),   nrounds = params_xgboost$nrounds )  model_lightgbm <- lightgbm_train(   data = x_train,   label = y_train,   params = list(     objective = \"binary\",     learning_rate = params_lightgbm$learning_rate,     num_iterations = params_lightgbm$num_iterations,     max_depth = params_lightgbm$max_depth,     num_leaves = 2^params_lightgbm$max_depth - 1   ),   verbose = -1 )  model_catboost <- catboost_train(   catboost_load_pool(data = x_train, label = y_train),   NULL,   params = list(     loss_function = \"Logloss\",     iterations = params_catboost$iterations,     depth = params_catboost$depth,     logging_level = \"Silent\"   ) )"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"xgboost","dir":"Articles","previous_headings":"Performance evaluation","what":"xgboost","title":"Model stacking for boosted trees","text":"","code":"roc_xgboost_train <- pROC::roc(   y_train,   predict(model_xgboost, x_train),   quiet = TRUE ) roc_xgboost_test <- pROC::roc(   y_test,   predict(model_xgboost, x_test),   quiet = TRUE ) roc_xgboost_train$auc #> Area under the curve: 0.9931 roc_xgboost_test$auc #> Area under the curve: 0.7827"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"lightgbm","dir":"Articles","previous_headings":"Performance evaluation","what":"lightgbm","title":"Model stacking for boosted trees","text":"","code":"roc_lightgbm_train <- pROC::roc(   y_train,   predict(model_lightgbm, x_train),   quiet = TRUE ) roc_lightgbm_test <- pROC::roc(   y_test,   predict(model_lightgbm, x_test),   quiet = TRUE ) roc_lightgbm_train$auc #> Area under the curve: 0.9915 roc_lightgbm_test$auc #> Area under the curve: 0.784"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"catboost","dir":"Articles","previous_headings":"Performance evaluation","what":"catboost","title":"Model stacking for boosted trees","text":"","code":"roc_catboost_train <- pROC::roc(   y_train,   catboost_predict(     model_catboost,     catboost_load_pool(data = x_train, label = NULL)   ),   quiet = TRUE ) roc_catboost_test <- pROC::roc(   y_test,   catboost_predict(     model_catboost,     catboost_load_pool(data = x_test, label = NULL)   ),   quiet = TRUE ) roc_catboost_train$auc #> Area under the curve: 0.9328 roc_catboost_test$auc #> Area under the curve: 0.7751"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"tabular-summary","dir":"Articles","previous_headings":"Performance evaluation","what":"Tabular summary","title":"Model stacking for boosted trees","text":"can summarize AUC values table. AUC values four models training testing set","code":""},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"roc-curves","dir":"Articles","previous_headings":"Performance evaluation","what":"ROC curves","title":"Model stacking for boosted trees","text":"Plot ROC curves models independent test set.","code":"pal <- c(\"#e15759\", \"#f28e2c\", \"#59a14f\", \"#4e79a7\", \"#76b7b2\")  plot(pROC::smooth(roc_stackgbm_test), col = pal[1], lwd = 1) plot(pROC::smooth(roc_xgboost_test), col = pal[2], lwd = 1, add = TRUE) plot(pROC::smooth(roc_lightgbm_test), col = pal[3], lwd = 1, add = TRUE) plot(pROC::smooth(roc_catboost_test), col = pal[4], lwd = 1, add = TRUE) legend(   \"bottomright\",   col = pal,   lwd = 2,   legend = c(\"stackgbm\", \"xgboost\", \"lightgbm\", \"catboost\") )"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"notes-on-categorical-features","dir":"Articles","previous_headings":"","what":"Notes on categorical features","title":"Model stacking for boosted trees","text":"xgboost lightgbm prefer categorical features encoded integers. catboost, categorical features can encoded character factors. avoid possible confusions, data categorical features, recommend converting integers use one-hot encoding, use numerical matrix input.","code":""},{"path":[]},{"path":"https://nanx.me/stackgbm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Nan Xiao. Author, maintainer, copyright holder.","code":""},{"path":"https://nanx.me/stackgbm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Xiao N (2024). stackgbm: Stacked Gradient Boosting Machines. R package version 0.1.0, https://github.com/nanxstats/stackgbm, https://nanx./stackgbm/.","code":"@Manual{,   title = {stackgbm: Stacked Gradient Boosting Machines},   author = {Nan Xiao},   year = {2024},   note = {R package version 0.1.0, https://github.com/nanxstats/stackgbm},   url = {https://nanx.me/stackgbm/}, }"},{"path":"https://nanx.me/stackgbm/index.html","id":"stackgbm-","dir":"","previous_headings":"","what":"Stacked Gradient Boosting Machines","title":"Stacked Gradient Boosting Machines","text":"stackgbm offers minimalist, research-oriented implementation model stacking (Wolpert, 1992) gradient boosted tree models built xgboost (Chen Guestrin, 2016), lightgbm (Ke et al., 2017), catboost (Prokhorenkova et al., 2018).","code":""},{"path":"https://nanx.me/stackgbm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Stacked Gradient Boosting Machines","text":"Install GitHub: install dependencies, check instructions manage dependencies.","code":"remotes::install_github(\"nanxstats/stackgbm\")"},{"path":"https://nanx.me/stackgbm/index.html","id":"model","dir":"","previous_headings":"","what":"Model","title":"Stacked Gradient Boosting Machines","text":"stackgbm implements classic two-layer stacking model: first layer generates ‚Äúfeatures‚Äù produced gradient boosting trees. second layer logistic regression uses features inputs.","code":""},{"path":"https://nanx.me/stackgbm/index.html","id":"related-projects","dir":"","previous_headings":"","what":"Related projects","title":"Stacked Gradient Boosting Machines","text":"comprehensive flexible implementation model stacking, see stacks tidymodels, mlr3pipelines mlr3, StackingClassifier scikit-learn.","code":""},{"path":"https://nanx.me/stackgbm/index.html","id":"code-of-conduct","dir":"","previous_headings":"","what":"Code of Conduct","title":"Stacked Gradient Boosting Machines","text":"Please note stackgbm project released Contributor Code Conduct. contributing project, agree abide terms.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_load_pool.html","id":null,"dir":"Reference","previous_headings":"","what":"Create a dataset ‚Äî catboost_load_pool","title":"Create a dataset ‚Äî catboost_load_pool","text":"Create dataset","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_load_pool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create a dataset ‚Äî catboost_load_pool","text":"","code":"catboost_load_pool(data, label = NULL, ...)"},{"path":"https://nanx.me/stackgbm/reference/catboost_load_pool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create a dataset ‚Äî catboost_load_pool","text":"data Predictors. label Labels. ... Additional parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_load_pool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create a dataset ‚Äî catboost_load_pool","text":"catboost.Pool object.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_load_pool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create a dataset ‚Äî catboost_load_pool","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr) #> catboost.Pool #> 80 rows, 10 columns catboost_load_pool(data = sim_data$x.tr, label = NULL) #> catboost.Pool #> 80 rows, 10 columns catboost_load_pool(data = sim_data$x.te, label = NULL) #> catboost.Pool #> 20 rows, 10 columns"},{"path":"https://nanx.me/stackgbm/reference/catboost_predict.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict based on the model ‚Äî catboost_predict","title":"Predict based on the model ‚Äî catboost_predict","text":"Predict based model","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_predict.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict based on the model ‚Äî catboost_predict","text":"","code":"catboost_predict(model, pool, prediction_type = \"Probability\", ...)"},{"path":"https://nanx.me/stackgbm/reference/catboost_predict.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict based on the model ‚Äî catboost_predict","text":"model trained model. pool dataset predict . prediction_type Prediction type. ... Additional parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_predict.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict based on the model ‚Äî catboost_predict","text":"Predicted values.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_predict.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict based on the model ‚Äî catboost_predict","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  x_train <- catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr) x_test <- catboost_load_pool(data = sim_data$x.te, label = NULL)  fit <- catboost_train(   x_train,   NULL,   params = list(     loss_function = \"Logloss\",     iterations = 100,     depth = 3,     logging_level = \"Silent\"   ) )  catboost_predict(fit, x_test) #>  [1] 0.6489797 0.6749884 0.3331358 0.5018237 0.7541328 0.3806107 0.2357996 #>  [8] 0.4752205 0.7186610 0.4027660 0.2351567 0.7293696 0.5726987 0.3565523 #> [15] 0.2649454 0.5513967 0.3885718 0.6271439 0.3863992 0.5583793"},{"path":"https://nanx.me/stackgbm/reference/catboost_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Train the model ‚Äî catboost_train","title":"Train the model ‚Äî catboost_train","text":"Train model","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train the model ‚Äî catboost_train","text":"","code":"catboost_train(learn_pool, test_pool = NULL, params = list())"},{"path":"https://nanx.me/stackgbm/reference/catboost_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train the model ‚Äî catboost_train","text":"learn_pool Training dataset. test_pool Testing dataset. params list training parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train the model ‚Äî catboost_train","text":"model object.","code":""},{"path":"https://nanx.me/stackgbm/reference/catboost_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train the model ‚Äî catboost_train","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  x_train <- catboost_load_pool(data = sim_data$x.tr, label = sim_data$y.tr)  fit <- catboost_train(   x_train,   NULL,   params = list(     loss_function = \"Logloss\",     iterations = 100,     depth = 3,     logging_level = \"Silent\"   ) )  fit #> CatBoost model (100 trees) #> Loss function: Logloss #> Fit to 10 feature(s)"},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":null,"dir":"Reference","previous_headings":"","what":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","text":"catboost - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","text":"","code":"cv_catboost(   x,   y,   params = cv_param_grid(),   n_folds = 5,   n_threads = 1,   seed = 42,   verbose = TRUE )"},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","text":"x Predictor matrix. y Response vector. params Parameter grid generated cv_param_grid(). n_folds Number folds. Default 5. n_threads number parallel threads. optimal speed, match number physical CPU cores, threads. See respective model documentation details. Default 1. seed Random seed reproducibility. verbose Show progress?","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_catboost","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  params <- cv_catboost(   sim_data$x.tr,   sim_data$y.tr,   params = cv_param_grid(     n_iterations = c(100, 200),     max_depth = c(3, 5),     learning_rate = c(0.1, 0.5)   ),   n_folds = 5,   n_threads = 1,   seed = 42,   verbose = FALSE )  params$df #>   iterations depth    metric #> 1        100     3 0.7788221 #> 2        200     3 0.8007519 #> 3        100     5 0.7907268 #> 4        200     5 0.7719298"},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","text":"lightgbm - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","text":"","code":"cv_lightgbm(   x,   y,   params = cv_param_grid(),   n_folds = 5,   n_threads = 1,   seed = 42,   verbose = TRUE )"},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","text":"x Predictor matrix. y Response vector. params Parameter grid generated cv_param_grid(). n_folds Number folds. Default 5. n_threads number parallel threads. optimal speed, match number physical CPU cores, threads. See respective model documentation details. Default 1. seed Random seed reproducibility. verbose Show progress?","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_lightgbm","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  params <- suppressWarnings(   cv_lightgbm(     sim_data$x.tr,     sim_data$y.tr,     params = cv_param_grid(       n_iterations = c(100, 200),       max_depth = c(3, 5),       learning_rate = c(0.1, 0.5)     ),     n_folds = 5,     n_threads = 1,     seed = 42,     verbose = FALSE   ) )  params$df #>   num_iterations max_depth learning_rate    metric #> 1            100         3           0.1 0.8421053 #> 2            200         3           0.1 0.8320802 #> 3            100         5           0.1 0.8421053 #> 4            200         5           0.1 0.8320802 #> 5            100         3           0.5 0.8082707 #> 6            200         3           0.5 0.8076441 #> 7            100         5           0.5 0.8082707 #> 8            200         5           0.5 0.8076441"},{"path":"https://nanx.me/stackgbm/reference/cv_param_grid.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","title":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","text":"function generates parameter grid used cross-validation gradient boosting decision tree (GBDT) models.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_param_grid.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","text":"","code":"cv_param_grid(   n_iterations = c(100, 200, 500, 1000),   max_depth = c(3, 5, 7, 9),   learning_rate = c(0.01, 0.05, 0.1, 0.2) )"},{"path":"https://nanx.me/stackgbm/reference/cv_param_grid.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","text":"n_iterations numeric vector number iterations (trees) GBDT model. equivalent nrounds XGBoost, num_iterations LightGBM, iterations CatBoost. max_depth numeric vector maximum tree depths. parameter equivalent max_depth XGBoost LightGBM, depth CatBoost. learning_rate numeric vector learning rates GBDT model. parameter equivalent eta XGBoost, learning_rate LightGBM, ignored CatBoost.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_param_grid.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","text":"list names parameter names values vectors possible values parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_param_grid.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate a parameter grid for cross-validation ‚Äî cv_param_grid","text":"","code":"params <- cv_param_grid(   n_iterations = c(10, 100),   max_depth = c(3, 5),   learning_rate = c(0.01, 0.1) )"},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","text":"xgboost - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","text":"","code":"cv_xgboost(   x,   y,   params = cv_param_grid(),   n_folds = 5,   n_threads = 1,   seed = 42,   verbose = TRUE )"},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","text":"x Predictor matrix. y Response vector. params Parameter grid generated cv_param_grid(). n_folds Number folds. Default 5. n_threads number parallel threads. optimal speed, match number physical CPU cores, threads. See respective model documentation details. Default 1. seed Random seed reproducibility. verbose Show progress?","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search ‚Äî cv_xgboost","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  params <- cv_xgboost(   sim_data$x.tr,   sim_data$y.tr,   params = cv_param_grid(     n_iterations = c(100, 200),     max_depth = c(3, 5),     learning_rate = c(0.1, 0.5)   ),   n_folds = 5,   n_threads = 1,   seed = 42,   verbose = FALSE )  params$df #>   nrounds max_depth eta    metric #> 1     100         3 0.1 0.7694236 #> 2     200         3 0.1 0.7888471 #> 3     100         5 0.1 0.7675439 #> 4     200         5 0.1 0.7775689 #> 5     100         3 0.5 0.7882206 #> 6     200         3 0.5 0.7957393 #> 7     100         5 0.5 0.7606516 #> 8     200         5 0.5 0.7662907"},{"path":"https://nanx.me/stackgbm/reference/is_installed_catboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Is catboost installed? ‚Äî is_installed_catboost","title":"Is catboost installed? ‚Äî is_installed_catboost","text":"catboost installed?","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_catboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Is catboost installed? ‚Äî is_installed_catboost","text":"","code":"is_installed_catboost()"},{"path":"https://nanx.me/stackgbm/reference/is_installed_catboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Is catboost installed? ‚Äî is_installed_catboost","text":"TRUE installed, FALSE .","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_catboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Is catboost installed? ‚Äî is_installed_catboost","text":"","code":"is_installed_catboost() #> [1] TRUE"},{"path":"https://nanx.me/stackgbm/reference/is_installed_lightgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Is lightgbm installed? ‚Äî is_installed_lightgbm","title":"Is lightgbm installed? ‚Äî is_installed_lightgbm","text":"lightgbm installed?","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_lightgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Is lightgbm installed? ‚Äî is_installed_lightgbm","text":"","code":"is_installed_lightgbm()"},{"path":"https://nanx.me/stackgbm/reference/is_installed_lightgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Is lightgbm installed? ‚Äî is_installed_lightgbm","text":"TRUE installed, FALSE .","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_lightgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Is lightgbm installed? ‚Äî is_installed_lightgbm","text":"","code":"is_installed_lightgbm() #> [1] TRUE"},{"path":"https://nanx.me/stackgbm/reference/is_installed_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"Is xgboost installed? ‚Äî is_installed_xgboost","title":"Is xgboost installed? ‚Äî is_installed_xgboost","text":"xgboost installed?","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Is xgboost installed? ‚Äî is_installed_xgboost","text":"","code":"is_installed_xgboost()"},{"path":"https://nanx.me/stackgbm/reference/is_installed_xgboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Is xgboost installed? ‚Äî is_installed_xgboost","text":"TRUE installed, FALSE .","code":""},{"path":"https://nanx.me/stackgbm/reference/is_installed_xgboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Is xgboost installed? ‚Äî is_installed_xgboost","text":"","code":"is_installed_xgboost() #> [1] TRUE"},{"path":"https://nanx.me/stackgbm/reference/lightgbm_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Train lightgbm model ‚Äî lightgbm_train","title":"Train lightgbm model ‚Äî lightgbm_train","text":"Train lightgbm model","code":""},{"path":"https://nanx.me/stackgbm/reference/lightgbm_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train lightgbm model ‚Äî lightgbm_train","text":"","code":"lightgbm_train(data, label, params, ...)"},{"path":"https://nanx.me/stackgbm/reference/lightgbm_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train lightgbm model ‚Äî lightgbm_train","text":"data Training data. label Labels. params list parameters. ... Additional parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/lightgbm_train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train lightgbm model ‚Äî lightgbm_train","text":"model object.","code":""},{"path":"https://nanx.me/stackgbm/reference/lightgbm_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train lightgbm model ‚Äî lightgbm_train","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  fit <- suppressWarnings(   lightgbm_train(     data = sim_data$x.tr,     label = sim_data$y.tr,     params = list(       objective = \"binary\",       learning_rate = 0.1,       num_iterations = 100,       max_depth = 3,       num_leaves = 2^3 - 1,       num_threads = 1     ),     verbose = -1   ) )  fit #> LightGBM Model (100 trees) #> Objective: binary #> Fitted to dataset with 10 columns"},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","title":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","text":"Make predictions stackgbm model object","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","text":"","code":"# S3 method for stackgbm predict(object, newx, threshold = 0.5, classes = c(1L, 0L), ...)"},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","text":"object stackgbm model object newx New predictor matrix threshold Decision threshold. Default 0.5. classes class encoding vector predicted outcome. naming order respected. ... unused","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","text":"list two vectors presenting predicted classification probabilities predicted response.","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make predictions from a stackgbm model object ‚Äî predict.stackgbm","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 1000,   p = 50,   rho = 0.6,   coef = rnorm(25, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  params_xgboost <- structure(   list(\"nrounds\" = 200, \"eta\" = 0.05, \"max_depth\" = 3),   class = c(\"cv_params\", \"cv_xgboost\") ) params_lightgbm <- structure(   list(\"num_iterations\" = 200, \"max_depth\" = 3, \"learning_rate\" = 0.05),   class = c(\"cv_params\", \"cv_lightgbm\") ) params_catboost <- structure(   list(\"iterations\" = 100, \"depth\" = 3),   class = c(\"cv_params\", \"cv_catboost\") )  fit <- stackgbm(   sim_data$x.tr,   sim_data$y.tr,   params = list(     params_xgboost,     params_lightgbm,     params_catboost   ) )  predict(fit, newx = sim_data$x.te) #> $prob #>   [1] 0.2558388 0.8338682 0.4763339 0.7455671 0.7555156 0.4501782 0.6574596 #>   [8] 0.6873576 0.6074110 0.7224988 0.6203971 0.1248982 0.1049472 0.8220409 #>  [15] 0.6851230 0.5846914 0.5835061 0.5298453 0.7929988 0.5336065 0.2955900 #>  [22] 0.5721034 0.5700517 0.3002608 0.6401235 0.1874141 0.7710912 0.1618248 #>  [29] 0.8386701 0.7695612 0.4402864 0.1992083 0.6331777 0.8200990 0.4218807 #>  [36] 0.2643347 0.8592997 0.7089676 0.7501508 0.7205304 0.2207929 0.7868003 #>  [43] 0.8154183 0.1770900 0.2803560 0.6606686 0.8833260 0.7457455 0.8560005 #>  [50] 0.7094953 0.3406476 0.1916315 0.7985752 0.1767514 0.3400872 0.4278666 #>  [57] 0.6809678 0.6107685 0.8609919 0.8872117 0.1540449 0.1187717 0.1254577 #>  [64] 0.1908997 0.3769435 0.8531140 0.5552780 0.2471893 0.7503111 0.1293596 #>  [71] 0.7032663 0.7448972 0.7595124 0.7960718 0.4391058 0.1525627 0.7314911 #>  [78] 0.2305579 0.7240893 0.2710769 0.4536461 0.8297858 0.4800380 0.7763404 #>  [85] 0.8802614 0.1624562 0.6632567 0.6769455 0.2806485 0.2053429 0.8185189 #>  [92] 0.5896262 0.1936939 0.7244081 0.3690217 0.8760344 0.1216465 0.3907955 #>  [99] 0.5570389 0.2036074 0.4433538 0.8425489 0.3575423 0.3402744 0.8941558 #> [106] 0.4540316 0.1979951 0.8648425 0.5952020 0.1428748 0.1353298 0.8469107 #> [113] 0.1137851 0.6688770 0.8405698 0.7519721 0.5995734 0.8981029 0.3791713 #> [120] 0.1964506 0.1208158 0.5446650 0.6865582 0.3288822 0.6390536 0.8760625 #> [127] 0.7542827 0.5631108 0.8979796 0.7153561 0.7489923 0.8415314 0.7956702 #> [134] 0.6311619 0.7230946 0.4559731 0.4489978 0.2873997 0.2933466 0.2392487 #> [141] 0.4506560 0.7583894 0.8599918 0.7691481 0.8316580 0.7875238 0.7244116 #> [148] 0.8302520 0.2416988 0.4758154 0.4926572 0.4272542 0.6020090 0.5788977 #> [155] 0.1144688 0.6329210 0.8999377 0.6190683 0.3689636 0.6348878 0.5609031 #> [162] 0.6726094 0.8749991 0.8718587 0.7209973 0.8046897 0.1514194 0.8438966 #> [169] 0.7098198 0.7401077 0.7184328 0.4137849 0.5942843 0.1694689 0.8406573 #> [176] 0.2947972 0.2482770 0.3138458 0.8490076 0.6213917 0.2340607 0.5202677 #> [183] 0.4311952 0.1190806 0.1396242 0.6837508 0.6065134 0.8807227 0.7196326 #> [190] 0.7747397 0.5394947 0.5879715 0.4708010 0.3819530 0.2171342 0.5964147 #> [197] 0.1617649 0.7513241 0.8646198 0.1626774 #>  #> $resp #>   [1] 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 #>  [38] 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 #>  [75] 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 #> [112] 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 #> [149] 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 #> [186] 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 #>"},{"path":"https://nanx.me/stackgbm/reference/stackgbm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stackgbm: Stacked Gradient Boosting Machines ‚Äî stackgbm-package","title":"stackgbm: Stacked Gradient Boosting Machines ‚Äî stackgbm-package","text":"minimalist implementation model stacking Wolpert (1992) doi:10.1016/S0893-6080(05)80023-1  boosted tree models. classic, two-layer stacking model implemented, first layer generates features using gradient boosting trees, second layer employs logistic regression model uses features inputs. Utilities training base models parameters tuning provided, allowing users experiment different ensemble configurations easily. aims provide simple efficient way combine multiple gradient boosting models improve predictive model performance robustness.","code":""},{"path":[]},{"path":"https://nanx.me/stackgbm/reference/stackgbm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stackgbm: Stacked Gradient Boosting Machines ‚Äî stackgbm-package","text":"Maintainer: Nan Xiao @nanx.(ORCID) [copyright holder]","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Model stacking for boosted trees ‚Äî stackgbm","title":"Model stacking for boosted trees ‚Äî stackgbm","text":"Model stacking two-layer architecture: first layer boosted tree models fitted xgboost, lightgbm, catboost; second layer logistic regression model.","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model stacking for boosted trees ‚Äî stackgbm","text":"","code":"stackgbm(x, y, params, n_folds = 5L, seed = 42, verbose = TRUE)"},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model stacking for boosted trees ‚Äî stackgbm","text":"x Predictor matrix. y Response vector. params list optimal parameter objects boosted tree models derived cv_xgboost(), cv_lightgbm(), cv_catboost(). order matter. n_folds Number folds. Default 5. seed Random seed reproducibility. verbose Show progress?","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model stacking for boosted trees ‚Äî stackgbm","text":"Fitted boosted tree models stacked tree model.","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model stacking for boosted trees ‚Äî stackgbm","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 1000,   p = 50,   rho = 0.6,   coef = rnorm(25, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  params_xgboost <- structure(   list(\"nrounds\" = 200, \"eta\" = 0.05, \"max_depth\" = 3),   class = c(\"cv_params\", \"cv_xgboost\") ) params_lightgbm <- structure(   list(\"num_iterations\" = 200, \"max_depth\" = 3, \"learning_rate\" = 0.05),   class = c(\"cv_params\", \"cv_lightgbm\") ) params_catboost <- structure(   list(\"iterations\" = 100, \"depth\" = 3),   class = c(\"cv_params\", \"cv_catboost\") )  fit <- stackgbm(   sim_data$x.tr,   sim_data$y.tr,   params = list(     params_xgboost,     params_lightgbm,     params_catboost   ) )  predict(fit, newx = sim_data$x.te) #> $prob #>   [1] 0.2558388 0.8338682 0.4763339 0.7455671 0.7555156 0.4501782 0.6574596 #>   [8] 0.6873576 0.6074110 0.7224988 0.6203971 0.1248982 0.1049472 0.8220409 #>  [15] 0.6851230 0.5846914 0.5835061 0.5298453 0.7929988 0.5336065 0.2955900 #>  [22] 0.5721034 0.5700517 0.3002608 0.6401235 0.1874141 0.7710912 0.1618248 #>  [29] 0.8386701 0.7695612 0.4402864 0.1992083 0.6331777 0.8200990 0.4218807 #>  [36] 0.2643347 0.8592997 0.7089676 0.7501508 0.7205304 0.2207929 0.7868003 #>  [43] 0.8154183 0.1770900 0.2803560 0.6606686 0.8833260 0.7457455 0.8560005 #>  [50] 0.7094953 0.3406476 0.1916315 0.7985752 0.1767514 0.3400872 0.4278666 #>  [57] 0.6809678 0.6107685 0.8609919 0.8872117 0.1540449 0.1187717 0.1254577 #>  [64] 0.1908997 0.3769435 0.8531140 0.5552780 0.2471893 0.7503111 0.1293596 #>  [71] 0.7032663 0.7448972 0.7595124 0.7960718 0.4391058 0.1525627 0.7314911 #>  [78] 0.2305579 0.7240893 0.2710769 0.4536461 0.8297858 0.4800380 0.7763404 #>  [85] 0.8802614 0.1624562 0.6632567 0.6769455 0.2806485 0.2053429 0.8185189 #>  [92] 0.5896262 0.1936939 0.7244081 0.3690217 0.8760344 0.1216465 0.3907955 #>  [99] 0.5570389 0.2036074 0.4433538 0.8425489 0.3575423 0.3402744 0.8941558 #> [106] 0.4540316 0.1979951 0.8648425 0.5952020 0.1428748 0.1353298 0.8469107 #> [113] 0.1137851 0.6688770 0.8405698 0.7519721 0.5995734 0.8981029 0.3791713 #> [120] 0.1964506 0.1208158 0.5446650 0.6865582 0.3288822 0.6390536 0.8760625 #> [127] 0.7542827 0.5631108 0.8979796 0.7153561 0.7489923 0.8415314 0.7956702 #> [134] 0.6311619 0.7230946 0.4559731 0.4489978 0.2873997 0.2933466 0.2392487 #> [141] 0.4506560 0.7583894 0.8599918 0.7691481 0.8316580 0.7875238 0.7244116 #> [148] 0.8302520 0.2416988 0.4758154 0.4926572 0.4272542 0.6020090 0.5788977 #> [155] 0.1144688 0.6329210 0.8999377 0.6190683 0.3689636 0.6348878 0.5609031 #> [162] 0.6726094 0.8749991 0.8718587 0.7209973 0.8046897 0.1514194 0.8438966 #> [169] 0.7098198 0.7401077 0.7184328 0.4137849 0.5942843 0.1694689 0.8406573 #> [176] 0.2947972 0.2482770 0.3138458 0.8490076 0.6213917 0.2340607 0.5202677 #> [183] 0.4311952 0.1190806 0.1396242 0.6837508 0.6065134 0.8807227 0.7196326 #> [190] 0.7747397 0.5394947 0.5879715 0.4708010 0.3819530 0.2171342 0.5964147 #> [197] 0.1617649 0.7513241 0.8646198 0.1626774 #>  #> $resp #>   [1] 0 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 1 1 0 0 1 1 0 0 1 #>  [38] 1 1 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 1 #>  [75] 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 1 0 0 #> [112] 1 0 1 1 1 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 1 1 1 1 1 1 1 #> [149] 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 #> [186] 1 1 1 1 1 1 1 0 0 0 1 0 1 1 0 #>"},{"path":"https://nanx.me/stackgbm/reference/xgboost_dmatrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","title":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","text":"Create xgb.DMatrix object","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_dmatrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","text":"","code":"xgboost_dmatrix(data, label = NULL, ...)"},{"path":"https://nanx.me/stackgbm/reference/xgboost_dmatrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","text":"data Matrix file. label Labels (optional). ... Additional parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_dmatrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","text":"xgb.DMatrix object.","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_dmatrix.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create xgb.DMatrix object ‚Äî xgboost_dmatrix","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  xgboost_dmatrix(sim_data$x.tr, label = sim_data$y.tr) #> xgb.DMatrix  dim: 80 x 10  info: label  colnames: no xgboost_dmatrix(sim_data$x.te) #> xgb.DMatrix  dim: 20 x 10  info: NA  colnames: no"},{"path":"https://nanx.me/stackgbm/reference/xgboost_train.html","id":null,"dir":"Reference","previous_headings":"","what":"Train xgboost model ‚Äî xgboost_train","title":"Train xgboost model ‚Äî xgboost_train","text":"Train xgboost model","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_train.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Train xgboost model ‚Äî xgboost_train","text":"","code":"xgboost_train(params, data, nrounds, ...)"},{"path":"https://nanx.me/stackgbm/reference/xgboost_train.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Train xgboost model ‚Äî xgboost_train","text":"params list parameters. data Training data. nrounds Maximum number boosting iterations. ... Additional parameters.","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_train.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Train xgboost model ‚Äî xgboost_train","text":"model object.","code":""},{"path":"https://nanx.me/stackgbm/reference/xgboost_train.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Train xgboost model ‚Äî xgboost_train","text":"","code":"sim_data <- msaenet::msaenet.sim.binomial(   n = 100,   p = 10,   rho = 0.6,   coef = rnorm(5, mean = 0, sd = 10),   snr = 1,   p.train = 0.8,   seed = 42 )  x_train <- xgboost_dmatrix(sim_data$x.tr, label = sim_data$y.tr)  fit <- xgboost_train(   params = list(     objective = \"binary:logistic\",     eval_metric = \"auc\",     max_depth = 3,     eta = 0.1   ),   data = x_train,   nrounds = 100,   nthread = 1 )  fit #> ##### xgb.Booster #> raw: 100.1 Kb  #> call: #>   xgboost::xgb.train(params = list(objective = \"binary:logistic\",  #>     eval_metric = \"auc\", max_depth = 3, eta = 0.1), data = <pointer: 0x55734167aec0>,  #>     nrounds = 100, nthread = 1) #> params (as set within xgb.train): #>   objective = \"binary:logistic\", eval_metric = \"auc\", max_depth = \"3\", eta = \"0.1\", nthread = \"1\", validate_parameters = \"TRUE\" #> xgb.attributes: #>   niter #> callbacks: #>   cb.print.evaluation(period = print_every_n) #> niter: 100 #> nfeatures : 10"},{"path":[]},{"path":"https://nanx.me/stackgbm/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"stackgbm 0.1.0","text":"First public release.","code":""}]
