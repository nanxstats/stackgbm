#' xgboost - parameter tuning and model selection with k-fold cross-validation
#' and grid search
#'
#' @param x Predictor matrix.
#' @param y Response vector.
#' @param params Parameter grid generated by [cv_param_grid()].
#' @param n_folds Number of folds. Default is 5.
#' @param n_threads The number of parallel threads for
#'   fitting individual models. Default is 1.
#'
#' @return
#' A data frame containing the complete tuning grid and the AUC values,
#' with the best parameter combination and the highest AUC value.
#'
#' @importFrom pROC auc
#' @importFrom foreach foreach
#' @importFrom doFuture "%dofuture%"
#' @importFrom progressr progressor
#'
#' @export
#'
#' @examplesIf is_installed_xgboost()
#' library(doFuture)
#' library(progressr)
#'
#' sim_data <- msaenet::msaenet.sim.binomial(
#'   n = 100,
#'   p = 10,
#'   rho = 0.6,
#'   coef = rnorm(5, mean = 0, sd = 10),
#'   snr = 1,
#'   p.train = 0.8,
#'   seed = 42
#' )
#'
#' set.seed(42)
#' plan(sequential)
#'
#' params <- cv_xgboost(
#'   sim_data$x.tr,
#'   sim_data$y.tr,
#'   params = cv_param_grid(
#'     n_iterations = c(100, 200),
#'     max_depth = c(3, 5),
#'     learning_rate = c(0.1, 0.5)
#'   ),
#'   n_folds = 5,
#'   n_threads = 1
#' )
#'
#' params$df
cv_xgboost <- function(
    x, y,
    params = cv_param_grid(),
    n_folds = 5,
    n_threads = 1) {
  params <- map_params_xgboost(params)

  nrow_x <- nrow(x)
  idx_shuffle <- sample(rep_len(seq_len(n_folds), nrow_x))
  df_grid <- expand.grid(
    "nrounds" = params$nrounds,
    "max_depth" = params$max_depth,
    "eta" = params$eta,
    "metric" = NA
  )
  idx_grid <- seq_len(nrow(df_grid))
  pb <- progressor(along = idx_grid)

  x <- as.matrix(x)

  pred_probs_list <- foreach(
    idx_param_set = idx_grid,
    .errorhandling = "pass",
    .options.future = list(seed = TRUE)
  ) %dofuture% {
    pb(sprintf("Param set: %d", idx_param_set))

    pred_probs <- matrix(NA, ncol = 2L, nrow = nrow_x)
    colnames(pred_probs) <- c("label", "prob")

    for (idx_fold in seq_len(n_folds)) {
      idx_train <- idx_shuffle != idx_fold
      idx_test <- idx_shuffle == idx_fold

      x_train <- x[idx_train, , drop = FALSE]
      y_train <- y[idx_train]
      x_test <- x[idx_test, , drop = FALSE]
      y_test <- y[idx_test]

      x_train <- xgboost_dmatrix(x_train, label = y_train)
      x_test <- xgboost_dmatrix(x_test)

      fit <- xgboost_train(
        params = list(
          objective = "binary:logistic",
          eval_metric = "auc",
          max_depth = df_grid[idx_param_set, "max_depth"],
          eta = df_grid[idx_param_set, "eta"]
        ),
        data = x_train,
        nrounds = df_grid[idx_param_set, "nrounds"],
        nthread = n_threads
      )

      pred_probs[idx_test, "label"] <- y_test
      pred_probs[idx_test, "prob"] <- predict(fit, x_test)
    }

    pred_probs
  }

  for (idx_param_set in idx_grid) {
    df_grid[idx_param_set, "metric"] <- as.numeric(pROC::auc(
      pred_probs_list[[idx_param_set]][, "label"],
      pred_probs_list[[idx_param_set]][, "prob"],
      quiet = TRUE
    ))
  }

  best_row <- which.max(df_grid$metric)
  best_metric <- df_grid$metric[best_row]
  best_nrounds <- df_grid$nrounds[best_row]
  best_eta <- df_grid$eta[best_row]
  best_max_depth <- df_grid$max_depth[best_row]

  structure(
    list(
      "df" = df_grid,
      "metric" = best_metric,
      "nrounds" = best_nrounds,
      "eta" = best_eta,
      "max_depth" = best_max_depth
    ),
    class = c("cv_params", "cv_xgboost")
  )
}
