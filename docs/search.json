[{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model Stacking for Boosted Trees","text":"Model stacking (Wolpert 1992) method ensemble learning combines strength multiple base learners drive predictive performance. particularly popular effective strategy used machine learning competitions. stackgbm implements two-layer stacking model: first layer generates “features” produced gradient boosting trees. boosted tree models built xgboost (Chen Guestrin 2016), lightgbm (Ke et al. 2017), catboost (Prokhorenkova et al. 2018). second layer logistic regression uses features inputs.","code":"library(\"stackgbm\") library(\"msaenet\") library(\"pROC\")"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"generate-data","dir":"Articles","previous_headings":"","what":"Generate data","title":"Model Stacking for Boosted Trees","text":"Let’s generate data demo. simulated data \\(1000 \\times 50\\) predictor matrix binary outcome vector. 800 samples training set rest 200 (independent) test set. 25 50 features informative follows \\(N(0, 10)\\).","code":"dat <- msaenet.sim.binomial(   n = 1000, p = 50, rho = 0.6,   coef = rnorm(25, 0, 10), snr = 1, p.train = 0.8,   seed = 1001 )  x_tr <- dat$x.tr y_tr <- as.vector(dat$y.tr) x_te <- dat$x.te y_te <- as.vector(dat$y.te)"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"parameter-tuning","dir":"Articles","previous_headings":"","what":"Parameter tuning","title":"Model Stacking for Boosted Trees","text":"cv_xgboost(), cv_lightgbm() cv_catboost() provide wrappers tuning essential hyperparameters type boosted tree models k-fold cross-validation. “optimal” parameters used fit stacking model later.","code":"params_xgb <- cv_xgboost(x_tr, y_tr) params_lgb <- cv_lightgbm(x_tr, y_tr) params_cat <- cv_catboost(x_tr, y_tr)"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"training","dir":"Articles","previous_headings":"","what":"Training","title":"Model Stacking for Boosted Trees","text":"","code":"model_stack <- stackgbm(   dat$x.tr, dat$y.tr,   params = list(     xgb.nrounds = params_xgb$nrounds,     xgb.learning_rate = params_xgb$learning_rate,     xgb.max_depth = params_xgb$max_depth,     lgb.num_iterations = params_lgb$num_iterations,     lgb.max_depth = params_lgb$max_depth,     lgb.learning_rate = params_lgb$learning_rate,     cat.iterations = params_cat$iterations,     cat.depth = params_cat$depth   ) ) #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function. #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function. #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function. #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function. #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function."},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"inference","dir":"Articles","previous_headings":"","what":"Inference","title":"Model Stacking for Boosted Trees","text":"","code":"roc_stack_tr <- roc(y_tr, predict(model_stack, x_tr)$prob, quiet = TRUE) roc_stack_te <- roc(y_te, predict(model_stack, x_te)$prob, quiet = TRUE) roc_stack_tr$auc #> Area under the curve: 0.9955 roc_stack_te$auc #> Area under the curve: 0.789"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"performance-benchmarking","dir":"Articles","previous_headings":"","what":"Performance benchmarking","title":"Model Stacking for Boosted Trees","text":"Let’s compare predictive performance stacking model three types tree boosting models (base learners) fitted individually: xgboost: lightgbm: catboost: Summarize AUC values table: AUC values four models training testing set Plot ROC curves independent test set:","code":"library(\"xgboost\") library(\"lightgbm\") library(\"catboost\")  model_xgb <- xgb.train(   params = list(     objective = \"binary:logistic\",     eval_metric = \"auc\",     max_depth = params_xgb$max_depth,     eta = params_xgb$learning_rate   ),   data = xgb.DMatrix(x_tr, label = y_tr),   nrounds = params_xgb$nrounds )  model_lgb <- lightgbm(   data = x_tr,   label = y_tr,   objective = \"binary\",   learning_rate = params_lgb$learning_rate,   num_iterations = params_lgb$num_iterations,   max_depth = params_lgb$max_depth,   num_leaves = 2^params_lgb$max_depth - 1,   verbose = -1 ) #> Warning in (function (params = list(), data, nrounds = 100L, valids = list(), : #> lgb.train: Found the following passed through '...': objective, learning_rate, #> num_iterations, max_depth, num_leaves. These will be used, but in future #> releases of lightgbm, this warning will become an error. Add these to 'params' #> instead. See ?lgb.train for documentation on how to call this function.  model_cat <- catboost.train(   catboost.load_pool(data = x_tr, label = y_tr), NULL,   params = list(     loss_function = \"Logloss\",     iterations = params_cat$iterations,     depth = params_cat$depth,     logging_level = \"Silent\"   ) ) roc_xgb_tr <- roc(y_tr, predict(model_xgb, x_tr), quiet = TRUE) roc_xgb_te <- roc(y_te, predict(model_xgb, x_te), quiet = TRUE) roc_xgb_tr$auc #> Area under the curve: 1 roc_xgb_te$auc #> Area under the curve: 0.7736 roc_lgb_tr <- roc(y_tr, predict(model_lgb, x_tr), quiet = TRUE) roc_lgb_te <- roc(y_te, predict(model_lgb, x_te), quiet = TRUE) roc_lgb_tr$auc #> Area under the curve: 0.9202 roc_lgb_te$auc #> Area under the curve: 0.7626 roc_cat_tr <- roc(y_tr, catboost.predict(model_cat, catboost.load_pool(data = x_tr, label = NULL)), quiet = TRUE) roc_cat_te <- roc(y_te, catboost.predict(model_cat, catboost.load_pool(data = x_te, label = NULL)), quiet = TRUE) roc_cat_tr$auc #> Area under the curve: 0.992 roc_cat_te$auc #> Area under the curve: 0.7757 library(\"ggsci\") pal <- pal_aaas()(4)  plot(smooth(roc_stack_te), col = pal[1]) plot(smooth(roc_xgb_te), col = pal[2], add = TRUE) plot(smooth(roc_lgb_te), col = pal[3], add = TRUE) plot(smooth(roc_cat_te), col = pal[4], add = TRUE) legend(\"bottomright\", legend = c(\"stackgbm\", \"xgboost\", \"lightgbm\", \"catboost\"), col = pal, lwd = 2)"},{"path":"https://nanx.me/stackgbm/articles/stackgbm.html","id":"notes-on-categorical-features","dir":"Articles","previous_headings":"","what":"Notes on categorical features","title":"Model Stacking for Boosted Trees","text":"xgboost lightgbm prefer categorical features encoded integers. catboost, categorical features can encoded character factors. avoid possible confusions, data categorical features, recommend converting integers use one-hot encoding, use numerical matrix input.","code":""},{"path":[]},{"path":"https://nanx.me/stackgbm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Nan Xiao. Author, maintainer.","code":""},{"path":"https://nanx.me/stackgbm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Xiao N (2021). stackgbm: Stacked Gradient Boosting Machines. https://nanx./stackgbm/, https://github.com/nanxstats/stackgbm.","code":"@Manual{,   title = {stackgbm: Stacked Gradient Boosting Machines},   author = {Nan Xiao},   year = {2021},   note = {https://nanx.me/stackgbm/, https://github.com/nanxstats/stackgbm}, }"},{"path":"https://nanx.me/stackgbm/index.html","id":"stackgbm-","dir":"","previous_headings":"","what":"Stacked Gradient Boosting Machines","title":"Stacked Gradient Boosting Machines","text":"stackgbm offers minimalist implementation model stacking (Wolpert, 1992) gradient boosted tree models built xgboost (Chen Guestrin, 2016), lightgbm (Ke et al., 2017), catboost (Prokhorenkova et al., 2018).","code":""},{"path":"https://nanx.me/stackgbm/index.html","id":"install","dir":"","previous_headings":"","what":"Install","title":"Stacked Gradient Boosting Machines","text":"First, install R package catboost yet available CRAN December 2020. Follow official installation guide. install stackgbm GitHub:","code":"remotes::install_github(\"nanxstats/stackgbm\")"},{"path":"https://nanx.me/stackgbm/index.html","id":"design","dir":"","previous_headings":"","what":"Design","title":"Stacked Gradient Boosting Machines","text":"stackgbm implements classic two-layer stacking model: first layer generates “features” produced gradient boosting trees. second layer logistic regression uses features inputs. code derived 2nd place solution precisionFDA brain cancer machine learning challenge 2020. make sure package easy understand, modify, extend, choose build package base R without special frameworks dialects. also exposed essential tunable parameters boosted tree models (learning rate, maximum depth tree, number iterations).","code":""},{"path":"https://nanx.me/stackgbm/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Stacked Gradient Boosting Machines","text":"stackgbm free open source software, licensed GPL-3.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":null,"dir":"Reference","previous_headings":"","what":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","text":"catboost - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","text":"","code":"cv_catboost(   x,   y,   nfolds = 5L,   seed = 42,   verbose = TRUE,   iterations = c(10, 50, 100, 200, 500, 1000),   depth = c(2, 3, 4, 5),   ncpus = parallel::detectCores() )"},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","text":"x Predictor matrix y Response vector nfolds Number folds. Default 5. seed Random seed reproducibility verbose Show progress? iterations Grid vector parameter iteractions. depth Grid vector parameter depth. ncpus Number CPU cores use. Defaults detectable cores.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_catboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"catboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_catboost","text":"","code":"# check the vignette for code examples"},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","text":"lightgbm - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","text":"","code":"cv_lightgbm(   x,   y,   nfolds = 5L,   seed = 42,   verbose = TRUE,   num_iterations = c(10, 50, 100, 200, 500, 1000),   max_depth = c(2, 3, 4, 5),   learning_rate = c(0.001, 0.01, 0.02, 0.05, 0.1),   ncpus = parallel::detectCores() )"},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","text":"x Predictor matrix y Response vector nfolds Number folds. Default 5. seed Random seed reproducibility verbose Show progress? num_iterations Grid vector parameter num_iterations. max_depth Grid vector parameter max_depth. learning_rate Grid vector parameter learning_rate. ncpus Number CPU cores use. Defaults detectable cores.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_lightgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"lightgbm - parameter tuning and model selection with k-fold cross-validation and grid search — cv_lightgbm","text":"","code":"# check the vignette for code examples"},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":null,"dir":"Reference","previous_headings":"","what":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","text":"xgboost - parameter tuning model selection k-fold cross-validation grid search","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","text":"","code":"cv_xgboost(   x,   y,   nfolds = 5L,   seed = 42,   verbose = TRUE,   nrounds = c(10, 50, 100, 200, 500, 1000),   max_depth = c(2, 3, 4, 5),   learning_rate = c(0.001, 0.01, 0.02, 0.05, 0.1),   ncpus = parallel::detectCores() )"},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","text":"x Predictor matrix y Response vector nfolds Number folds. Default 5. seed Random seed reproducibility verbose Show progress? nrounds Grid vector parameter nrounds. max_depth Grid vector parameter max_depth. learning_rate Grid vector parameter learning_rate. ncpus Number CPU cores use. Defaults detectable cores.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","text":"data frame containing complete tuning grid AUC values, best parameter combination highest AUC value.","code":""},{"path":"https://nanx.me/stackgbm/reference/cv_xgboost.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"xgboost - parameter tuning and model selection with k-fold cross-validation and grid search — cv_xgboost","text":"","code":"# check the vignette for code examples"},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Make predictions from a stackgbm model object — predict.stackgbm","title":"Make predictions from a stackgbm model object — predict.stackgbm","text":"Make predictions stackgbm model object","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Make predictions from a stackgbm model object — predict.stackgbm","text":"","code":"# S3 method for stackgbm predict(object, newx, threshold = 0.5, classes = c(1L, 0L), ...)"},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Make predictions from a stackgbm model object — predict.stackgbm","text":"object stackgbm model object newx New predictor matrix threshold Decision threshold. Default 0.5. classes class encoding vector predicted outcome. naming order respected. ... unused","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Make predictions from a stackgbm model object — predict.stackgbm","text":"list two vectors presenting predicted classification probabilities predicted response.","code":""},{"path":"https://nanx.me/stackgbm/reference/predict.stackgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Make predictions from a stackgbm model object — predict.stackgbm","text":"","code":"# check the vignette for code examples"},{"path":"https://nanx.me/stackgbm/reference/stackgbm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"stackgbm: Stacked Gradient Boosting Machines — stackgbm-package","title":"stackgbm: Stacked Gradient Boosting Machines — stackgbm-package","text":"minimalist implementation model stacking boosted tree models built 'xgboost', 'lightgbm', 'catboost'.","code":""},{"path":[]},{"path":"https://nanx.me/stackgbm/reference/stackgbm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"stackgbm: Stacked Gradient Boosting Machines — stackgbm-package","text":"Maintainer: Nan Xiao @nanx.(ORCID)","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":null,"dir":"Reference","previous_headings":"","what":"Model stacking for boosted trees — stackgbm","title":"Model stacking for boosted trees — stackgbm","text":"Model stacking two-layer architecture: first layer boosted tree models fitted xgboost, lightgbm, catboost; second layer logistic regression model.","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Model stacking for boosted trees — stackgbm","text":"","code":"stackgbm(x, y, params, nfolds = 5L, seed = 42, verbose = TRUE)"},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Model stacking for boosted trees — stackgbm","text":"x Predictor matrix y Response vector params list optimal parameters boosted tree models. Can derived cv_xgboost, cv_lightgbm, cv_catboost. nfolds Number folds. Default 5. seed Random seed reproducibility verbose Show progress?","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Model stacking for boosted trees — stackgbm","text":"Fitted boosted tree models stacked tree model.","code":""},{"path":"https://nanx.me/stackgbm/reference/stackgbm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Model stacking for boosted trees — stackgbm","text":"","code":"# check the vignette for code examples"},{"path":[]},{"path":"https://nanx.me/stackgbm/news/index.html","id":"new-features-0-1-0","dir":"Changelog","previous_headings":"","what":"New Features","title":"stackgbm 0.1.0","text":"First public release.","code":""}]
